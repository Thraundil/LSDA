{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOWNLOAD_AMOUNT = 500 # set to none for unlimited \n",
    "LABELS_PATH = '../data/labels/'\n",
    "LABEL_CNT = 228\n",
    "IMG_SIZE = 299\n",
    "OUT_FILE = '../data/images.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import zipfile\n",
    "import json\n",
    "import h5py\n",
    "import urllib3\n",
    "import multiprocessing\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from urllib3.util import Retry\n",
    "urllib3.disable_warnings()\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load image urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(fname, path, max_parse=None):\n",
    "    \"\"\"\n",
    "    If the given filename does not exist, unzips a file called \"<fname>.zip\"\n",
    "    \"\"\"\n",
    "    if not os.path.exists(fname):\n",
    "        # unzip first\n",
    "        with zipfile.ZipFile(path + fname + '.zip',\"r\") as zip_ref:\n",
    "            zip_ref.extractall(path)\n",
    "            \n",
    "    ids_urls = []\n",
    "    ids_labels = []\n",
    "    with open(path + fname, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        for image in data[\"images\"]:\n",
    "            url = image[\"url\"]\n",
    "            id = image[\"imageId\"]\n",
    "            ids_urls.append((id, url))\n",
    "        if \"annotations\" in data.keys():\n",
    "            for image in data[\"annotations\"]:\n",
    "                label_list = np.array(list(map(int, image[\"labelId\"])))\n",
    "                label_list = label_list - 1\n",
    "                id = image[\"imageId\"]\n",
    "                label_vector = np.zeros(LABEL_CNT, dtype=np.int8)\n",
    "                label_vector[label_list] = 1\n",
    "                ids_labels.append((id, label_vector))\n",
    "    \n",
    "    if max_parse is not None:\n",
    "        ids_urls = ids_urls[:max_parse]\n",
    "        ids_labels = ids_labels[:max_parse]\n",
    "        \n",
    "    return ids_urls, ids_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ids_urls, train_ids_labels = parse('train.json', LABELS_PATH, max_parse=DOWNLOAD_AMOUNT)\n",
    "val_ids_urls, val_ids_labels = parse('validation.json', LABELS_PATH, max_parse=DOWNLOAD_AMOUNT)\n",
    "test_ids_urls, _ = parse('test.json', LABELS_PATH, max_parse=DOWNLOAD_AMOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_shape = (len(train_ids_urls), 299, 299, 3)\n",
    "val_shape = (len(val_ids_urls), 299, 299, 3)\n",
    "test_shape = (len(test_ids_urls), 299, 299, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file = h5py.File(OUT_FILE, mode='w')\n",
    "hdf5_file.create_dataset(\"train_img\", train_shape, np.uint8, chunks=True)\n",
    "hdf5_file.create_dataset(\"val_img\", val_shape, np.uint8, chunks=True)\n",
    "hdf5_file.create_dataset(\"test_img\", test_shape, np.uint8, chunks=True)\n",
    "hdf5_file.create_dataset(\"train_labels\", (len(train_ids_urls), LABEL_CNT), np.int8)\n",
    "hdf5_file[\"train_labels\"][...] = [labels for (id, labels) in train_ids_labels]\n",
    "hdf5_file.create_dataset(\"val_labels\", (len(val_ids_urls), LABEL_CNT), np.int8)\n",
    "hdf5_file[\"val_labels\"][...] = [labels for (id, labels) in val_ids_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_image(id_url):\n",
    "    id, url = id_url\n",
    "    http = urllib3.PoolManager(retries=Retry(connect=3, read=2, redirect=3))\n",
    "    response = http.request(\"GET\", url)\n",
    "    image = Image.open(io.BytesIO(response.data))\n",
    "    image = image.resize((IMG_SIZE, IMG_SIZE))\n",
    "    image_rgb = image.convert(\"RGB\")\n",
    "    return (id, np.array(image_rgb))\n",
    "    \n",
    "\n",
    "def download(ids_urls, h5_dataset):\n",
    "    pool = multiprocessing.Pool(processes=30)\n",
    "    with tqdm(total=len(ids_urls)) as progress_bar:\n",
    "        for id, img in pool.imap_unordered(download_image, ids_urls):\n",
    "            h5_dataset[int(id) - 1, ...] = img\n",
    "            progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:08<00:00, 55.70it/s]\n",
      "100%|██████████| 500/500 [00:09<00:00, 55.01it/s]\n",
      "100%|██████████| 500/500 [00:12<00:00, 41.36it/s]\n"
     ]
    }
   ],
   "source": [
    "download(train_ids_urls, hdf5_file[\"train_img\"])\n",
    "download(val_ids_urls, hdf5_file[\"val_img\"])\n",
    "download(test_ids_urls, hdf5_file[\"test_img\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdf5_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
